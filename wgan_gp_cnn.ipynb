{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12ZoobQcHIPTbrdT8FFVPFX7xlk2na9BT",
      "authorship_tag": "ABX9TyOYXSt2/+WHfqGjGcbzucSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oyanrayring/HiAI-Engine/blob/master/wgan_gp_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4JpQvjmR2BtY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "IMAGE_DIR = 'drive/MyDrive/Colab Notebooks/acg_images/faces'\n",
        "os.listdir('drive/MyDrive/Colab Notebooks/acg_images')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_huCvycHna-",
        "outputId": "ad5e149a-79ff-44b0-e490-d32ddec2c7be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['faces']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练轮数\n",
        "N_EPOCHS = 3\n",
        "# 批处理数量\n",
        "BATCH_SIZE = 64\n",
        "# 图片大小\n",
        "IMAGE_SIZE = 96\n",
        "# 学习率\n",
        "LEARNING_RATE = 0.0002\n",
        "# b1\n",
        "BIAS_1 = 0.5\n",
        "# b2\n",
        "BIAS_2 = 0.999\n",
        "# 维度\n",
        "LATENT_DIM = 100\n",
        "# 通道数\n",
        "CHANNELS = 3\n",
        "# 生成器生成5次后，鉴别器优化1次\n",
        "N_CRITIC = 5\n",
        "# 每200个BATCH后输出一次样例数据\n",
        "SAMPLE_INTERVAL = 200"
      ],
      "metadata": {
        "id": "qWvJ1AJw4fM5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "V_38CeHq2pvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "lAfKyYwg4Wr_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SourceDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_path = os.listdir(self.root_dir)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_path[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image_data = Image.open(img_path).convert('RGB')\n",
        "        return transform(image_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_path)\n"
      ],
      "metadata": {
        "id": "6-oQH37V4Swu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    SourceDataset(IMAGE_DIR),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "IOdUvjcFNAhL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"images\", exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)"
      ],
      "metadata": {
        "id": "4cj5z4RR2xc8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "metadata": {
        "id": "S1i4UdZ6J7YD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.init_size = 12  # 计算得到的初始尺寸\n",
        "        self.l1 = nn.Sequential(nn.Linear(100, 128 * self.init_size ** 2))\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),  # 12x12 -> 24x24\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),  # 24x24 -> 48x48\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),  # 48x48 -> 96x96\n",
        "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "iqOYblwHMFs-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
        "            if bn:\n",
        "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            return block\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(3, 16, bn=False),\n",
        "            *discriminator_block(16, 32),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128)\n",
        "        )\n",
        "        ds_size = 96 // 2 ** 4  # 确保尺寸一致\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.model(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        return validity\n"
      ],
      "metadata": {
        "id": "HBC6TWbmMTeY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "if cuda:\n",
        "  generator.cuda()\n",
        "  discriminator.cuda()"
      ],
      "metadata": {
        "id": "sDqRsB39Map6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_gp = 10"
      ],
      "metadata": {
        "id": "IW2DdVOzMmWF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BIAS_1, BIAS_2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BIAS_1, BIAS_2))\n"
      ],
      "metadata": {
        "id": "9ZP_jf5GMnI9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "metadata": {
        "id": "UnZiDi_zOWoi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 梯度惩罚\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n"
      ],
      "metadata": {
        "id": "elyJQoh9OYEI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(\"logs/wgan_gp_cnn\")"
      ],
      "metadata": {
        "id": "dMHxiG-pR3E4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  batches_done = 0\n",
        "  start_time = int(time.time())\n",
        "  for epoch in range(N_EPOCHS):\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "      # Configure input\n",
        "      real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      optimizer_D.zero_grad()\n",
        "      # Sample noise as generator input\n",
        "      z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], LATENT_DIM))))\n",
        "      # Generate a batch of images\n",
        "      fake_imgs = generator(z)\n",
        "      # Real images\n",
        "      real_validity = discriminator(real_imgs)\n",
        "      # Fake images\n",
        "      fake_validity = discriminator(fake_imgs)\n",
        "      # Gradient penalty\n",
        "      gp = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
        "      # Adversarial loss\n",
        "      d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
        "      d_loss.backward()\n",
        "      optimizer_D.step()\n",
        "\n",
        "      optimizer_G.zero_grad()\n",
        "      # Train the generator every n_critic steps\n",
        "      if i % opt.n_critic == 0:\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z)\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        # Train on fake images\n",
        "        fake_validity = discriminator(fake_imgs)\n",
        "        g_loss = -torch.mean(fake_validity)\n",
        "        writer.add_scalars(\n",
        "          \"loss\",\n",
        "          {\"generator\": g_loss.item(), \"discriminator\": d_loss.item()},\n",
        "          batches_done\n",
        "        )\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{N_EPOCHS}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "        # 保存样例\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "          save_image(fake_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "          # tensorboard中展示的图片\n",
        "          writer.add_image(\"generate images\", fake_imgs[0], batches_done)\n",
        "\n",
        "        batches_done += opt.n_critic\n",
        "        pass\n",
        "    # end one epoch\n",
        "    print(f\"one cost {int(time.time()) - start_time} seconds .\\n\")\n",
        "    if epoch % 10 == 0:\n",
        "      # save model\n",
        "      torch.save(generator.state_dict(), f\"models/generator_{epoch}.pth\")\n",
        "      torch.save(discriminator.state_dict(), f\"models/discriminator_{epoch}.pth\")\n"
      ],
      "metadata": {
        "id": "K2GlIEreOlb0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()\n",
        "print(\"finished\")"
      ],
      "metadata": {
        "id": "HsOWabioRkZ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "e2172e3c-31a4-4d72-c61e-e261eb4ca959"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-119e4d817994>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-09126eab794f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# Train the generator every n_critic steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_critic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#  Train Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    }
  ]
}